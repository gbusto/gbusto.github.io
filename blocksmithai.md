---
title: Blocksmith AI
layout: page
---

## overview

this is a page to go over aspects of blocksmith that i feel comfortable sharing without giving too much away about how it works. i may at some point open source it or parts of it, but for now i'm keeping it as a closed saas app.

- [why i built this](#why-i-built-this)
- [why blocksmith is different](#why-blocksmith-is-different)
- [early prototype](#early-prototype)
- [model generation overview](#model-generation-overview)
- [texturing pipeline](#overview)
    - [why current tools fail](#overview)
    - [requirements i needed](#overview)
    - [progression and experiments](#overview)


### why i built this
as mentioned on the [games](./games.md) page, i built 2 hytopia games. the first one i built introduced me to an unexpected friction point: getting custom blocky style assets/models for my game. building the game was actually not that tough to do; it was harder to find assets for my game.

i found the best option at the time was meshy.ai and it's "voxel" style model generator. but it wasn't great. and i was a bit confused beween *voxel* and *blocky*. voxel style is a bunch of little cubes that make up a larger thing. blocky means using a single block per part of the model; like for a minecraft person, the head is a single block, the torso is a single block, etc. (what i actually end up building is a 3d "blocky" style model generator.) anyways, meshy seemed the closest and best option. i didn't know at the time or wasn't comfortable trying to download and host open source 3d models like hunyuan.

also, as i would find later, every single ai model, tool, or platform is designed around making high-poly, watertight meshes / 3d models. meaning it's a single monolithic mesh. they can't make true cuboid, blocky geometry. they can generate *approximate* cuboids, but it's all a single watertight mesh, the faces of cube aren't perfectly flat, the vertices aren't perfect 90-degree corners. and they're usually very high poly and relatively large files, which aren't ideal for a web-based game platform like hytopia.

also, texturing is usually baked into the overall process and does NOT support the really nice, truly pixelated look associated with blocky models / minecraft / hytopia. and every single texturing tool out there that *can* be run without a corresponding mesh generation pipeline is designed for higher resolution textures/atlases.

so, i saw this as a big gap in the market of ai and 3d. i also thought it would be very helpful to build for hytopia, because as more developers come to build on their platform, they'll need models for their games, and a tool that can generate models for them would probably incredibly useful. this is a community i really liked and wanted to support, and i saw this as a great way to do that.


### why blocksmith is different
i mentioned that (1) every state of the art ai model outputs a high poly mesh and can only *approximate* cuboid shapes, and (2) they output monolithic meshes. blocksmith was designed from the ground up to solve both of those issues.

here's an example of a model generated by meshy:
![Meshy AI](images/meshy-minecraft-person.png)

it actually looks pretty good; meshy seems state of the art. BUT, take a look at the number of faces and vertices. those are CRAZY numbers for a "blocky" model.

now take a look at a similar model from blocksmith:
![Blocksmith AI](images/blocksmith-minecraft-person.png)

by comparison, this one has 132 faces and 264 vertices.

i'm not able to download this model from meshy since it was using their latest model, but i believe it's still a monolithic, watertight mesh. whereas you can see the blocksmith model loaded into babylon is made up of nice, logically named parts and ready to be animated.


### early prototype
the earliest prototype of blocksmith created monolithic voxel style models. at the start of this whole process, i had ZERO experience in 3d, and ZERO experience with minecraft. i wasn't thinking about monolithic meshes or meshes composed of multiple parts, true cuboidal shapes, file size, faces and vertices counts, etc. those weren't even on my radar.

after lots of experimentation and brainstorming, i found a pipeline that worked:
- let a user upload a screenshot
- call trellis through the replicate.com api
- get back a 3d model
- use a script that i created by lots of trial and error with gemini that basically voxelizes the model by walking over it with some kind of marching cubes algorithm

it was super simple, but the first nugget of something interesting! [here's my x post](https://x.com/gabebusto/status/1907545086176424209) announcing the first version of it.

the coloring of the models in this version looked like some kind of airbrush technique.
![Asset Hero Model](images/assethero-model-image.png). a big part of the reason is that i wasn't using nearest filtering vs lanczos, i think. but i spent a long time trying to figure out how to solve this coloring.

i also got hints from the hytopia team engineers who were more well versed in this area, and they explained to me how and why these models are not optimized. so that sent me down the rabbit hole and i continued from there.

i eventually switched to a more advanced [generative process](https://x.com/gabebusto/status/1910100441519505819) that became the foundation of what it is today.


### model generation overview
as mentioned in the section above, i eventually discovered a more robust generative process to create block style models composed of several parts, as opposed voxel style, monolithic meshes. without revealing too much, i came up with a custom process that allows an llm to output a series of mesh parts that can be composed into a full model, with semantic node names and even allowing for a hierarchy to be created.

what was interesting about this is that i think if i was more well versed in the world of 3d and ai models and how to read/apply research, i probably would have spent time trying to come up with an over-engineered solution. the fact that i was so naive may have been a kind of superpower because i tried something that would probably seem dumb to some really smart researchers, but it just happened to work.

the result of this process is that i could (1) output a truly block style model that was ~optimized (minimal geometry) and ideal for web-based games (specifically hytopia), (2) with semantic node and group names, and (3) in a relatively short amount of time and with little compute power relative to SOTA 3d ai models.

this was HUGE because it meant that i could:
- easily animate these models given that they already had node names and groups you could target
    - OR, i could use ai to animate models (which i can already do)
- have fully semantic understanding of what regions of the texture atlas belong to which parts of the model
- easily target and edit specific parts of a model by e.g. prompting ai
- create a simple export process to convert the model to a `.bbmodel` file type so people could import it into blockbench (basically it's blender but for minecraft style assets, and much simpler to use and navigate)

here are some demos from my x profile:
- [multiple prompt-to-block models](https://x.com/gabebusto/status/1912313114231533990)
- [a person wearing a hat](https://x.com/gabebusto/status/1912318527584809469)
- [an xbox style controller](https://x.com/gabebusto/status/1912321084696801419)
- [a cute lil dog](https://x.com/gabebusto/status/1912320086104301822)

the texturing at this point was still VERY basic. but i learned a whole lot about it that i'll share below.

the version of the model generation pipeline that's live on blocksmith today (as of September 2025) is *very* similar, but has been improved and optimized to allow ai to create more highly detailed models while saving on output tokens.


### texturing pipeline

coming soon!